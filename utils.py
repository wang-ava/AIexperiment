"""
Fashion-MNISTæ•°æ®é›†åŠ è½½å·¥å…·
æ”¯æŒè¯»å–å’Œè§£å‹.gzæ ¼å¼çš„Fashion-MNISTæ•°æ®
æ•°æ®é»˜è®¤åŠ è½½åˆ° CPUï¼ˆNumPyï¼‰ï¼Œå¯åœ¨éœ€è¦æ—¶ä¼ è¾“åˆ° GPU
"""
import gzip
import numpy as np
import os


def set_random_seed(seed=42):
    """
    è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å®éªŒå¯é‡å¤æ€§
    
    å‚æ•°:
        seed: éšæœºç§å­å€¼
    """
    np.random.seed(seed)
    try:
        import cupy as cp
        cp.random.seed(seed)
        print(f"âœ“ éšæœºç§å­å·²è®¾ç½®: {seed} (NumPy + CuPy)")
    except ImportError:
        print(f"âœ“ éšæœºç§å­å·²è®¾ç½®: {seed} (NumPy)")


def im2col(X, kernel_h, kernel_w, stride=1, padding=0):
    """
    å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºåˆ—çŸ©é˜µï¼Œç”¨äºé«˜æ•ˆçš„å·ç§¯è®¡ç®—
    im2col æ–¹æ³•å°†å·ç§¯æ“ä½œè½¬æ¢ä¸ºçŸ©é˜µä¹˜æ³•
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜é€Ÿåº¦
    
    å‚æ•°:
        X: è¾“å…¥, shape (batch, channels, height, width)
        kernel_h: å·ç§¯æ ¸é«˜åº¦
        kernel_w: å·ç§¯æ ¸å®½åº¦
        stride: æ­¥é•¿
        padding: å¡«å……
    
    è¿”å›:
        col: åˆ—çŸ©é˜µ, shape (batch*out_h*out_w, channels*kernel_h*kernel_w)
        out_h, out_w: è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦
    """
    from gpu_utils import xp
    
    batch, channels, height, width = X.shape
    
    # æ·»åŠ padding
    if padding > 0:
        X = xp.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 
                  mode='constant')
        height += 2 * padding
        width += 2 * padding
    
    # è®¡ç®—è¾“å‡ºå°ºå¯¸
    out_h = (height - kernel_h) // stride + 1
    out_w = (width - kernel_w) // stride + 1
    
    # å‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨é«˜çº§ç´¢å¼•å‡å°‘å¾ªç¯
    col_height = batch * out_h * out_w
    col_width = channels * kernel_h * kernel_w
    col = xp.zeros((col_height, col_width), dtype=X.dtype)
    
    # ä½¿ç”¨å‘é‡åŒ–ç´¢å¼•æå–çª—å£
    # ä¸ºæ¯ä¸ªè¾“å‡ºä½ç½®ç”Ÿæˆç´¢å¼•
    col_idx = 0
    for b in range(batch):
        for oh in range(out_h):
            y_start = oh * stride
            y_end = y_start + kernel_h
            for ow in range(out_w):
                x_start = ow * stride
                x_end = x_start + kernel_w
                
                # æå–å½“å‰çª—å£çš„æ‰€æœ‰é€šé“ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
                window = X[b, :, y_start:y_end, x_start:x_end]  # (channels, kernel_h, kernel_w)
                col[col_idx, :] = window.flatten()
                col_idx += 1
    
    return col, out_h, out_w


def col2im(col, X_shape, kernel_h, kernel_w, stride=1, padding=0):
    """
    å°†åˆ—çŸ©é˜µè½¬æ¢å›å›¾åƒæ ¼å¼ï¼ˆim2colçš„é€†æ“ä½œï¼‰
    
    å‚æ•°:
        col: åˆ—çŸ©é˜µ
        X_shape: åŸå§‹è¾“å…¥çš„å½¢çŠ¶ (batch, channels, height, width)
        kernel_h: å·ç§¯æ ¸é«˜åº¦
        kernel_w: å·ç§¯æ ¸å®½åº¦
        stride: æ­¥é•¿
        padding: å¡«å……
    
    è¿”å›:
        X: å›¾åƒ, shape (batch, channels, height, width)
    """
    from gpu_utils import xp
    
    batch, channels, height, width = X_shape
    
    # è®¡ç®—è¾“å‡ºå°ºå¯¸
    out_h = (height + 2 * padding - kernel_h) // stride + 1
    out_w = (width + 2 * padding - kernel_w) // stride + 1
    
    col = col.reshape(batch, out_h, out_w, channels, kernel_h, kernel_w).transpose(0, 3, 4, 5, 1, 2)
    
    # åˆ›å»ºå¸¦paddingçš„å›¾åƒ
    X = xp.zeros((batch, channels, height + 2 * padding, width + 2 * padding))
    
    for y in range(kernel_h):
        y_max = y + stride * out_h
        for x in range(kernel_w):
            x_max = x + stride * out_w
            X[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]
    
    # ç§»é™¤padding
    if padding > 0:
        return X[:, :, padding:-padding, padding:-padding]
    return X


def load_mnist_images(filename):
    """
    åŠ è½½Fashion-MNISTå›¾åƒæ•°æ®
    
    å‚æ•°:
        filename: å›¾åƒæ–‡ä»¶è·¯å¾„ (.gzæ ¼å¼)
    
    è¿”å›:
        images: numpyæ•°ç»„, shapeä¸º(N, 784), Nä¸ºå›¾åƒæ•°é‡
    """
    with gzip.open(filename, 'rb') as f:
        # è¯»å–magic numberå’Œå…ƒæ•°æ®
        magic = int.from_bytes(f.read(4), 'big')
        num_images = int.from_bytes(f.read(4), 'big')
        rows = int.from_bytes(f.read(4), 'big')
        cols = int.from_bytes(f.read(4), 'big')
        
        # è¯»å–å›¾åƒæ•°æ®
        buf = f.read(rows * cols * num_images)
        data = np.frombuffer(buf, dtype=np.uint8)
        data = data.reshape(num_images, rows * cols)
        
    return data.astype(np.float32) / 255.0  # å½’ä¸€åŒ–åˆ°[0, 1]


def load_mnist_labels(filename):
    """
    åŠ è½½Fashion-MNISTæ ‡ç­¾æ•°æ®
    
    å‚æ•°:
        filename: æ ‡ç­¾æ–‡ä»¶è·¯å¾„ (.gzæ ¼å¼)
    
    è¿”å›:
        labels: numpyæ•°ç»„, shapeä¸º(N,), Nä¸ºæ ‡ç­¾æ•°é‡
    """
    with gzip.open(filename, 'rb') as f:
        # è¯»å–magic numberå’Œå…ƒæ•°æ®
        magic = int.from_bytes(f.read(4), 'big')
        num_labels = int.from_bytes(f.read(4), 'big')
        
        # è¯»å–æ ‡ç­¾æ•°æ®
        buf = f.read(num_labels)
        labels = np.frombuffer(buf, dtype=np.uint8)
        
    return labels


def load_fashion_mnist(data_dir='../dataset'):
    """
    åŠ è½½å®Œæ•´çš„Fashion-MNISTæ•°æ®é›†
    
    å‚æ•°:
        data_dir: æ•°æ®é›†ç›®å½•è·¯å¾„
    
    è¿”å›:
        (train_images, train_labels, test_images, test_labels)
    """
    train_images = load_mnist_images(
        os.path.join(data_dir, 'train-images-idx3-ubyte.gz'))
    train_labels = load_mnist_labels(
        os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'))
    test_images = load_mnist_images(
        os.path.join(data_dir, 'test-images-idx3-ubyte.gz'))
    test_labels = load_mnist_labels(
        os.path.join(data_dir, 'test-labels-idx1-ubyte.gz'))
    
    return train_images, train_labels, test_images, test_labels


def one_hot_encode(labels, num_classes=10):
    """
    å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
    
    å‚æ•°:
        labels: æ ‡ç­¾æ•°ç»„
        num_classes: ç±»åˆ«æ•°é‡
    
    è¿”å›:
        one_hot: one-hotç¼–ç çš„æ ‡ç­¾, shapeä¸º(N, num_classes)
    """
    one_hot = np.zeros((labels.size, num_classes))
    one_hot[np.arange(labels.size), labels] = 1
    return one_hot


def create_mini_batches(X, y, batch_size):
    """
    åˆ›å»ºmini-batch
    
    å‚æ•°:
        X: è¾“å…¥æ•°æ®
        y: æ ‡ç­¾
        batch_size: batchå¤§å°
    
    è¿”å›:
        batches: [(X_batch, y_batch), ...]
    """
    m = X.shape[0]
    batches = []
    
    # æ‰“ä¹±æ•°æ®
    permutation = np.random.permutation(m)
    X_shuffled = X[permutation]
    y_shuffled = y[permutation]
    
    # åˆ›å»ºå®Œæ•´çš„batches
    num_complete_batches = m // batch_size
    for k in range(num_complete_batches):
        X_batch = X_shuffled[k * batch_size:(k + 1) * batch_size]
        y_batch = y_shuffled[k * batch_size:(k + 1) * batch_size]
        batches.append((X_batch, y_batch))
    
    # å¤„ç†å‰©ä½™æ•°æ®
    if m % batch_size != 0:
        X_batch = X_shuffled[num_complete_batches * batch_size:]
        y_batch = y_shuffled[num_complete_batches * batch_size:]
        batches.append((X_batch, y_batch))
    
    return batches


# Fashion-MNISTç±»åˆ«åç§°
CLASS_NAMES = [
    'T-shirt/top',  # 0
    'Trouser',      # 1
    'Pullover',     # 2
    'Dress',        # 3
    'Coat',         # 4
    'Sandal',       # 5
    'Shirt',        # 6
    'Sneaker',      # 7
    'Bag',          # 8
    'Ankle boot'    # 9
]


def get_class_name(label):
    """è·å–ç±»åˆ«åç§°"""
    return CLASS_NAMES[label]


def generate_training_report(model_name, history, train_acc, test_acc, X_train, y_train, 
                             X_test, y_test, model, layer_info, learning_rate, training_time=None):
    """
    ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå¹¶ä¿å­˜åˆ°æ–‡ä»¶
    
    å‚æ•°:
        model_name: æ¨¡å‹åç§°
        history: è®­ç»ƒå†å²
        train_acc: æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡
        test_acc: æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_test, y_test: æµ‹è¯•æ•°æ®
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        layer_info: ç½‘ç»œå±‚ä¿¡æ¯
        learning_rate: å­¦ä¹ ç‡
        training_time: è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    import numpy as np
    from datetime import datetime
    
    # åˆ›å»ºæŠ¥å‘Šå†…å®¹
    report_lines = []
    
    def add_line(text):
        """æ·»åŠ ä¸€è¡Œåˆ°æŠ¥å‘Šå¹¶æ‰“å°"""
        print(text)
        report_lines.append(text)
    
    add_line("\n" + "=" * 70)
    add_line(" " * 20 + f"{model_name} è®­ç»ƒæŠ¥å‘Š")
    add_line("=" * 70)
    
    # æ·»åŠ ç”Ÿæˆæ—¶é—´
    add_line(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. æ¨¡å‹é…ç½®ä¿¡æ¯
    add_line("\nã€æ¨¡å‹é…ç½®ã€‘")
    add_line("-" * 70)
    add_line(f"  ç½‘ç»œç»“æ„: {layer_info}")
    add_line(f"  è®­ç»ƒè½®æ•°: {history['epochs']}")
    add_line(f"  æ‰¹æ¬¡å¤§å°: {history['batch_size']}")
    add_line(f"  å­¦ä¹ ç‡: {learning_rate}")
    if training_time:
        add_line(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
    
    # 2. æ•°æ®é›†ä¿¡æ¯
    add_line("\nã€æ•°æ®é›†ä¿¡æ¯ã€‘")
    add_line("-" * 70)
    add_line(f"  è®­ç»ƒæ ·æœ¬æ•°: {X_train.shape[0]:,}")
    add_line(f"  æµ‹è¯•æ ·æœ¬æ•°: {X_test.shape[0]:,}")
    add_line(f"  è¾“å…¥ç»´åº¦: {X_train.shape[1]}")
    add_line(f"  ç±»åˆ«æ•°é‡: 10")
    
    # 3. è®­ç»ƒè¿‡ç¨‹
    add_line("\nã€è®­ç»ƒè¿‡ç¨‹ã€‘")
    add_line("-" * 70)
    add_line(f"  åˆå§‹è®­ç»ƒå‡†ç¡®ç‡: {history['train_acc'][0]:.4f} ({history['train_acc'][0]*100:.2f}%)")
    add_line(f"  åˆå§‹æµ‹è¯•å‡†ç¡®ç‡: {history['test_acc'][0]:.4f} ({history['test_acc'][0]*100:.2f}%)")
    add_line(f"  æœ€é«˜è®­ç»ƒå‡†ç¡®ç‡: {max(history['train_acc']):.4f} ({max(history['train_acc'])*100:.2f}%)")
    add_line(f"  æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡: {max(history['test_acc']):.4f} ({max(history['test_acc'])*100:.2f}%)")
    add_line(f"  å‡†ç¡®ç‡æå‡: {(history['test_acc'][-1] - history['test_acc'][0]):.4f} "
          f"({(history['test_acc'][-1] - history['test_acc'][0])*100:.2f}%)")
    
    # æ·»åŠ æ¯ä¸ªepochçš„è¯¦ç»†æ•°æ®
    add_line("\n  å„è½®æ¬¡è¯¦ç»†æ•°æ®:")
    add_line("  " + "-" * 50)
    add_line(f"  {'Epoch':<8} {'è®­ç»ƒå‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15}")
    add_line("  " + "-" * 50)
    for i in range(len(history['train_acc'])):
        add_line(f"  {i+1:<8} {history['train_acc'][i]:.4f} ({history['train_acc'][i]*100:.2f}%)   "
                f"{history['test_acc'][i]:.4f} ({history['test_acc'][i]*100:.2f}%)")
    
    # 4. æœ€ç»ˆæ€§èƒ½
    add_line("\nã€æœ€ç»ˆæ€§èƒ½ã€‘")
    add_line("-" * 70)
    add_line(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)")
    add_line(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
    add_line(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {(train_acc - test_acc):.4f} ({(train_acc - test_acc)*100:.2f}%)")
    
    # 5. æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
    add_line("\nã€å„ç±»åˆ«æ€§èƒ½ã€‘")
    add_line("-" * 70)
    predictions = model.predict(X_test)
    class_correct = np.zeros(10)
    class_total = np.zeros(10)
    
    for i in range(len(y_test)):
        label = y_test[i]
        class_total[label] += 1
        if predictions[i] == label:
            class_correct[label] += 1
    
    add_line(f"  {'ç±»åˆ«':<15} {'å‡†ç¡®ç‡':<10} {'æ­£ç¡®/æ€»æ•°'}")
    add_line("  " + "-" * 40)
    for i in range(10):
        acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        add_line(f"  {get_class_name(i):<15} {acc:.4f}     {int(class_correct[i])}/{int(class_total[i])}")
    
    # 6. é¢„æµ‹ç¤ºä¾‹
    add_line("\nã€é¢„æµ‹ç¤ºä¾‹ã€‘")
    add_line("-" * 70)
    sample_indices = np.random.choice(len(X_test), 10, replace=False)
    correct_count = 0
    for idx in sample_indices:
        pred = model.predict(X_test[idx:idx+1])[0]
        true_label = y_test[idx]
        is_correct = pred == true_label
        if is_correct:
            correct_count += 1
        status = "âœ“" if is_correct else "âœ—"
        add_line(f"  æ ·æœ¬ #{idx:5d}: çœŸå®={get_class_name(true_label):15s} | "
              f"é¢„æµ‹={get_class_name(pred):15s} {status}")
    add_line(f"\n  éšæœºæ ·æœ¬å‡†ç¡®ç‡: {correct_count}/10")
    
    # 7. æ€»ç»“
    add_line("\nã€æ€»ç»“ã€‘")
    add_line("-" * 70)
    if test_acc >= 0.90:
        performance = "ä¼˜ç§€"
    elif test_acc >= 0.85:
        performance = "è‰¯å¥½"
    elif test_acc >= 0.80:
        performance = "ä¸€èˆ¬"
    else:
        performance = "éœ€è¦æ”¹è¿›"
    
    add_line(f"  æ¨¡å‹æ€§èƒ½è¯„çº§: {performance}")
    add_line(f"  è®­ç»ƒçŠ¶æ€: {'å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ' if (train_acc - test_acc) > 0.05 else 'è®­ç»ƒæ­£å¸¸'}")
    
    suggestion_text = ""
    if (train_acc - test_acc) > 0.05:
        suggestion_text = "è€ƒè™‘æ·»åŠ æ­£åˆ™åŒ–æˆ–dropoutæ¥å‡å°‘è¿‡æ‹Ÿåˆ"
    elif test_acc < 0.85:
        suggestion_text = "å¯ä»¥å°è¯•å¢åŠ ç½‘ç»œæ·±åº¦ã€è°ƒæ•´å­¦ä¹ ç‡æˆ–è®­ç»ƒæ›´å¤šè½®æ¬¡"
    else:
        suggestion_text = "æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨"
    add_line(f"  å»ºè®®: {suggestion_text}")
    
    add_line("\n" + "=" * 70)
    add_line(" " * 25 + "æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    add_line("=" * 70 + "\n")
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    # åˆ›å»ºreportsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    reports_dir = "reports"
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šæ¨¡å‹åç§°_æ—¶é—´æˆ³.txt
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    # æ¸…ç†æ¨¡å‹åç§°ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
    clean_model_name = model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')
    filename = os.path.join(reports_dir, f"{clean_model_name}_{timestamp}.txt")
    
    # å†™å…¥æ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}\n")
    
    return filename  # è¿”å›æŠ¥å‘Šæ–‡ä»¶è·¯å¾„


def parse_report_file(report_file):
    """
    ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è§£æè®­ç»ƒç»“æœ
    
    å‚æ•°:
        report_file: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        åŒ…å«è§£æç»“æœçš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    try:
        with open(report_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        result = {}
        
        # è§£ææµ‹è¯•å‡†ç¡®ç‡
        import re
        test_acc_match = re.search(r'æµ‹è¯•å‡†ç¡®ç‡:\s*([\d.]+)\s*\(([\d.]+)%\)', content)
        if test_acc_match:
            result['test_acc'] = float(test_acc_match.group(1))
        
        # è§£æè®­ç»ƒå‡†ç¡®ç‡
        train_acc_match = re.search(r'è®­ç»ƒå‡†ç¡®ç‡:\s*([\d.]+)\s*\(([\d.]+)%\)', content)
        if train_acc_match:
            result['train_acc'] = float(train_acc_match.group(1))
        
        # è§£æè®­ç»ƒæ—¶é—´
        time_match = re.search(r'è®­ç»ƒæ—¶é—´:\s*([\d.]+)\s*ç§’', content)
        if time_match:
            result['training_time'] = float(time_match.group(1))
        
        return result if result else None
    except Exception:
        return None


def generate_summary_report(model_results, reports_dir="reports"):
    """
    ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        model_results: å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹çš„ç»“æœä¿¡æ¯
            [{
                'model_name': æ¨¡å‹åç§°,
                'train_acc': è®­ç»ƒå‡†ç¡®ç‡,
                'test_acc': æµ‹è¯•å‡†ç¡®ç‡,
                'training_time': è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰,
                'status': 'success' æˆ– 'failed',
                'error': é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
            }, ...]
        reports_dir: æŠ¥å‘Šç›®å½•
    """
    from datetime import datetime
    import glob
    
    # åˆ›å»ºæŠ¥å‘Šå†…å®¹
    report_lines = []
    
    def add_line(text):
        """æ·»åŠ ä¸€è¡Œåˆ°æŠ¥å‘Šå¹¶æ‰“å°"""
        print(text)
        report_lines.append(text)
    
    add_line("\n" + "=" * 70)
    add_line(" " * 20 + "æ‰€æœ‰æ¨¡å‹è®­ç»ƒæ±‡æ€»æŠ¥å‘Š")
    add_line("=" * 70)
    
    # æ·»åŠ ç”Ÿæˆæ—¶é—´
    add_line(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_models = len(model_results)
    success_models = [r for r in model_results if r.get('status') == 'success']
    failed_models = [r for r in model_results if r.get('status') == 'failed']
    
    add_line("\nã€è¿è¡Œç»Ÿè®¡ã€‘")
    add_line("-" * 70)
    add_line(f"  æ€»æ¨¡å‹æ•°: {total_models}")
    add_line(f"  æˆåŠŸ: {len(success_models)} ä¸ª")
    add_line(f"  å¤±è´¥: {len(failed_models)} ä¸ª")
    
    # æˆåŠŸæ¨¡å‹å¯¹æ¯”
    if success_models:
        add_line("\nã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‘")
        add_line("-" * 70)
        add_line(f"  {'æ¨¡å‹åç§°':<25} {'è®­ç»ƒå‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15} {'è®­ç»ƒæ—¶é—´':<15}")
        add_line("  " + "-" * 68)
        
        # æŒ‰æµ‹è¯•å‡†ç¡®ç‡æ’åº
        sorted_models = sorted(success_models, key=lambda x: x.get('test_acc', 0), reverse=True)
        
        for result in sorted_models:
            model_name = result.get('model_name', 'æœªçŸ¥')
            train_acc = result.get('train_acc', 0)
            test_acc = result.get('test_acc', 0)
            training_time = result.get('training_time', 0)
            
            train_acc_str = f"{train_acc:.4f} ({train_acc*100:.2f}%)"
            test_acc_str = f"{test_acc:.4f} ({test_acc*100:.2f}%)"
            
            if training_time > 0:
                if training_time < 60:
                    time_str = f"{training_time:.1f}ç§’"
                else:
                    time_str = f"{training_time/60:.1f}åˆ†é’Ÿ"
            else:
                time_str = "æœªçŸ¥"
            
            add_line(f"  {model_name:<25} {train_acc_str:<15} {test_acc_str:<15} {time_str:<15}")
        
        # æœ€ä½³æ¨¡å‹
        if sorted_models:
            best = sorted_models[0]
            add_line(f"\n  ğŸ† æœ€ä½³æ¨¡å‹: {best.get('model_name')} (æµ‹è¯•å‡†ç¡®ç‡: {best.get('test_acc', 0):.4f})")
    
    # å¤±è´¥æ¨¡å‹
    if failed_models:
        add_line("\nã€å¤±è´¥æ¨¡å‹ã€‘")
        add_line("-" * 70)
        for result in failed_models:
            model_name = result.get('model_name', 'æœªçŸ¥')
            error = result.get('error', 'æœªçŸ¥é”™è¯¯')
            add_line(f"  âœ— {model_name}: {error}")
    
    # å°è¯•ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è¡¥å……å‡†ç¡®ç‡ä¿¡æ¯
    if os.path.exists(reports_dir):
        report_files = sorted(glob.glob(os.path.join(reports_dir, "*.txt")), 
                             key=os.path.getmtime, reverse=True)
        
        # ä¸ºæ¯ä¸ªæˆåŠŸæ¨¡å‹æŸ¥æ‰¾å¯¹åº”çš„æŠ¥å‘Šæ–‡ä»¶å¹¶è§£æ
        model_file_map = {}
        for file in report_files:
            basename = os.path.basename(file)
            # è·³è¿‡æ±‡æ€»æŠ¥å‘Š
            if 'æ±‡æ€»æŠ¥å‘Š' in basename:
                continue
            # ä»æ–‡ä»¶åæå–æ¨¡å‹åç§°
            parts = basename.rsplit('_', 2)
            if len(parts) >= 3:
                model_key = '_'.join(parts[:-2])
                if model_key not in model_file_map:
                    model_file_map[model_key] = file
        
        # æ›´æ–°æ¨¡å‹ç»“æœ
        for result in model_results:
            if result.get('status') == 'success':
                model_name = result.get('model_name', '')
                # å°è¯•åŒ¹é…æ¨¡å‹åç§°
                for key, file_path in model_file_map.items():
                    if key in model_name or model_name in key:
                        parsed = parse_report_file(file_path)
                        if parsed:
                            if 'test_acc' in parsed and result.get('test_acc', 0) == 0:
                                result['test_acc'] = parsed['test_acc']
                            if 'train_acc' in parsed and result.get('train_acc', 0) == 0:
                                result['train_acc'] = parsed['train_acc']
                            if 'training_time' in parsed and result.get('training_time', 0) == 0:
                                result['training_time'] = parsed['training_time']
                        break
    
    # æŠ¥å‘Šæ–‡ä»¶åˆ—è¡¨
    add_line("\nã€ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶ã€‘")
    add_line("-" * 70)
    
    # æŸ¥æ‰¾æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶
    if os.path.exists(reports_dir):
        report_files = sorted(glob.glob(os.path.join(reports_dir, "*.txt")), 
                             key=os.path.getmtime, reverse=True)
        
        # åªæ˜¾ç¤ºæœ€è¿‘çš„æŠ¥å‘Šï¼ˆæ¯ä¸ªæ¨¡å‹ä¸€ä¸ªï¼‰
        model_files = {}
        for file in report_files:
            # ä»æ–‡ä»¶åæå–æ¨¡å‹åç§°ï¼ˆå»æ‰æ—¶é—´æˆ³ï¼‰
            basename = os.path.basename(file)
            # è·³è¿‡æ±‡æ€»æŠ¥å‘Šï¼ˆä¼šåœ¨æœ€åå•ç‹¬æ˜¾ç¤ºï¼‰
            if 'æ±‡æ€»æŠ¥å‘Š' in basename:
                continue
            # æ‰¾åˆ°æœ€åä¸€ä¸ªä¸‹åˆ’çº¿çš„ä½ç½®ï¼ˆæ—¶é—´æˆ³å‰ï¼‰
            parts = basename.rsplit('_', 2)
            if len(parts) >= 3:
                model_key = '_'.join(parts[:-2])  # å»æ‰æ—¶é—´æˆ³éƒ¨åˆ†
                if model_key not in model_files:
                    model_files[model_key] = file
        
        if model_files:
            for model_key, file_path in sorted(model_files.items()):
                file_size = os.path.getsize(file_path)
                file_size_kb = file_size / 1024
                add_line(f"  âœ“ {os.path.basename(file_path)} ({file_size_kb:.1f} KB)")
        else:
            add_line("  æœªæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶")
    else:
        add_line("  æŠ¥å‘Šç›®å½•ä¸å­˜åœ¨")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°æ–‡ä»¶
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_filename = os.path.join(reports_dir, f"æ±‡æ€»æŠ¥å‘Š_{timestamp}.txt")
    
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    # åœ¨æŠ¥å‘Šä¸­æ·»åŠ æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶ä¿¡æ¯
    add_line(f"\n  ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {os.path.basename(summary_filename)}")
    
    add_line("\n" + "=" * 70)
    add_line(" " * 25 + "æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    add_line("=" * 70 + "\n")
    
    print(f"\nâœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_filename}\n")
    
    return summary_filename

