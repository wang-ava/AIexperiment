"""
Fashion-MNISTæ•°æ®é›†åŠ è½½å·¥å…·
æ”¯æŒè¯»å–å’Œè§£å‹.gzæ ¼å¼çš„Fashion-MNISTæ•°æ®
æ•°æ®é»˜è®¤åŠ è½½åˆ° CPUï¼ˆNumPyï¼‰ï¼Œå¯åœ¨éœ€è¦æ—¶ä¼ è¾“åˆ° GPU (PyTorch)
"""
import gzip
import numpy as np
import os
import torch


def set_random_seed(seed=42):
    """
    è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å®éªŒå¯é‡å¤æ€§ï¼ˆPyTorchç‰ˆæœ¬ï¼‰
    
    å‚æ•°:
        seed: éšæœºç§å­å€¼
    """
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"âœ“ éšæœºç§å­å·²è®¾ç½®: {seed} (NumPy + PyTorch)")


# æ³¨æ„ï¼šPyTorch ä½¿ç”¨å†…ç½®çš„å·ç§¯æ“ä½œï¼Œä¸å†éœ€è¦ im2col å’Œ col2im å‡½æ•°
# ä¿ç•™è¿™äº›å‡½æ•°ä½œä¸ºå ä½ç¬¦ä»¥ä¿æŒå‘åå…¼å®¹ï¼Œä½†å®ƒä»¬ä¸ä¼šè¢«ä½¿ç”¨
def im2col(X, kernel_h, kernel_w, stride=1, padding=0):
    """
    å·²å¼ƒç”¨ï¼šPyTorchä½¿ç”¨å†…ç½®å·ç§¯æ“ä½œï¼Œä¸å†éœ€è¦im2col
    ä¿ç•™æ­¤å‡½æ•°ä»…ä¸ºå‘åå…¼å®¹
    """
    raise NotImplementedError("im2col is not needed in PyTorch. Use torch.nn.Conv2d instead.")


def col2im(col, X_shape, kernel_h, kernel_w, stride=1, padding=0):
    """
    å·²å¼ƒç”¨ï¼šPyTorchä½¿ç”¨å†…ç½®å·ç§¯æ“ä½œï¼Œä¸å†éœ€è¦col2im
    ä¿ç•™æ­¤å‡½æ•°ä»…ä¸ºå‘åå…¼å®¹
    """
    raise NotImplementedError("col2im is not needed in PyTorch. Use torch.nn.ConvTranspose2d instead.")


def load_mnist_images(filename):
    """
    åŠ è½½Fashion-MNISTå›¾åƒæ•°æ®
    
    å‚æ•°:
        filename: å›¾åƒæ–‡ä»¶è·¯å¾„ (.gzæ ¼å¼)
    
    è¿”å›:
        images: numpyæ•°ç»„, shapeä¸º(N, 784), Nä¸ºå›¾åƒæ•°é‡
    """
    with gzip.open(filename, 'rb') as f:
        # è¯»å–magic numberå’Œå…ƒæ•°æ®
        magic = int.from_bytes(f.read(4), 'big')
        num_images = int.from_bytes(f.read(4), 'big')
        rows = int.from_bytes(f.read(4), 'big')
        cols = int.from_bytes(f.read(4), 'big')
        
        # è¯»å–å›¾åƒæ•°æ®
        buf = f.read(rows * cols * num_images)
        data = np.frombuffer(buf, dtype=np.uint8)
        data = data.reshape(num_images, rows * cols)
        
    return data.astype(np.float32) / 255.0  # å½’ä¸€åŒ–åˆ°[0, 1]


def load_mnist_labels(filename):
    """
    åŠ è½½Fashion-MNISTæ ‡ç­¾æ•°æ®
    
    å‚æ•°:
        filename: æ ‡ç­¾æ–‡ä»¶è·¯å¾„ (.gzæ ¼å¼)
    
    è¿”å›:
        labels: numpyæ•°ç»„, shapeä¸º(N,), Nä¸ºæ ‡ç­¾æ•°é‡
    """
    with gzip.open(filename, 'rb') as f:
        # è¯»å–magic numberå’Œå…ƒæ•°æ®
        magic = int.from_bytes(f.read(4), 'big')
        num_labels = int.from_bytes(f.read(4), 'big')
        
        # è¯»å–æ ‡ç­¾æ•°æ®
        buf = f.read(num_labels)
        labels = np.frombuffer(buf, dtype=np.uint8)
        
    return labels


def load_fashion_mnist(data_dir=None):
    """
    åŠ è½½å®Œæ•´çš„Fashion-MNISTæ•°æ®é›†
    
    å‚æ•°:
        data_dir: æ•°æ®é›†ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
    
    è¿”å›:
        (train_images, train_labels, test_images, test_labels)
    """
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†è·¯å¾„
    if data_dir is None:
        # è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # å°è¯•å‡ ä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            os.path.join(current_dir, 'dataset'),  # åŒç›®å½•ä¸‹çš„dataset
            os.path.join(current_dir, '..', 'dataset'),  # ä¸Šçº§ç›®å½•ä¸‹çš„dataset
            './dataset',  # ç›¸å¯¹è·¯å¾„
            '../dataset'  # ä¸Šçº§ç›¸å¯¹è·¯å¾„
        ]
        
        for path in possible_paths:
            test_file = os.path.join(path, 'train-images-idx3-ubyte.gz')
            if os.path.exists(test_file):
                data_dir = path
                break
        
        if data_dir is None:
            raise FileNotFoundError(
                f"æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•ã€‚è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å­˜åœ¨äºä»¥ä¸‹è·¯å¾„ä¹‹ä¸€ï¼š\n" +
                "\n".join([f"  - {os.path.abspath(p)}" for p in possible_paths])
            )
    
    # Fashion-MNISTæ ‡å‡†å‘½åä½¿ç”¨t10kå‰ç¼€ï¼Œä¸æ˜¯test
    train_images = load_mnist_images(
        os.path.join(data_dir, 'train-images-idx3-ubyte.gz'))
    train_labels = load_mnist_labels(
        os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'))
    # å°è¯•ä¸¤ç§å‘½åæ–¹å¼ï¼šå…ˆå°è¯•t10kï¼ˆæ ‡å‡†å‘½åï¼‰ï¼Œå†å°è¯•testï¼ˆå…¼å®¹æ€§ï¼‰
    test_images_path = os.path.join(data_dir, 't10k-images-idx3-ubyte.gz')
    if not os.path.exists(test_images_path):
        test_images_path = os.path.join(data_dir, 'test-images-idx3-ubyte.gz')
    test_images = load_mnist_images(test_images_path)
    
    test_labels_path = os.path.join(data_dir, 't10k-labels-idx1-ubyte.gz')
    if not os.path.exists(test_labels_path):
        test_labels_path = os.path.join(data_dir, 'test-labels-idx1-ubyte.gz')
    test_labels = load_mnist_labels(test_labels_path)
    
    return train_images, train_labels, test_images, test_labels


def one_hot_encode(labels, num_classes=10):
    """
    å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
    
    å‚æ•°:
        labels: æ ‡ç­¾æ•°ç»„
        num_classes: ç±»åˆ«æ•°é‡
    
    è¿”å›:
        one_hot: one-hotç¼–ç çš„æ ‡ç­¾, shapeä¸º(N, num_classes)
    """
    one_hot = np.zeros((labels.size, num_classes))
    one_hot[np.arange(labels.size), labels] = 1
    return one_hot


def create_mini_batches(X, y, batch_size):
    """
    åˆ›å»ºmini-batchï¼ˆPyTorchç‰ˆæœ¬ï¼‰
    æ”¯æŒnumpyæ•°ç»„æˆ–PyTorchå¼ é‡
    
    å‚æ•°:
        X: è¾“å…¥æ•°æ®ï¼ˆnumpyæ•°ç»„æˆ–PyTorchå¼ é‡ï¼‰
        y: æ ‡ç­¾ï¼ˆnumpyæ•°ç»„ï¼‰
        batch_size: batchå¤§å°
    
    è¿”å›:
        batches: [(X_batch, y_batch), ...]ï¼Œè¿”å›numpyæ•°ç»„æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
    """
    import numpy as np
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå¦‚æœè¾“å…¥æ˜¯PyTorchå¼ é‡ï¼‰
    if isinstance(X, torch.Tensor):
        X = X.cpu().numpy()
    if isinstance(y, torch.Tensor):
        y = y.cpu().numpy()
    
    X = np.asarray(X)
    y = np.asarray(y)
    
    m = X.shape[0]
    batches = []
    
    # æ‰“ä¹±æ•°æ® - ç”Ÿæˆéšæœºç´¢å¼•
    permutation = np.random.permutation(m)
    X_shuffled = X[permutation]
    y_shuffled = y[permutation]
    
    # åˆ›å»ºå®Œæ•´çš„batches
    num_complete_batches = m // batch_size
    for k in range(num_complete_batches):
        X_batch = X_shuffled[k * batch_size:(k + 1) * batch_size]
        y_batch = y_shuffled[k * batch_size:(k + 1) * batch_size]
        batches.append((X_batch, y_batch))
    
    # å¤„ç†å‰©ä½™æ•°æ®
    if m % batch_size != 0:
        X_batch = X_shuffled[num_complete_batches * batch_size:]
        y_batch = y_shuffled[num_complete_batches * batch_size:]
        batches.append((X_batch, y_batch))
    
    return batches


# Fashion-MNISTç±»åˆ«åç§°
CLASS_NAMES = [
    'T-shirt/top',  # 0
    'Trouser',      # 1
    'Pullover',     # 2
    'Dress',        # 3
    'Coat',         # 4
    'Sandal',       # 5
    'Shirt',        # 6
    'Sneaker',      # 7
    'Bag',          # 8
    'Ankle boot'    # 9
]


def get_class_name(label):
    """è·å–ç±»åˆ«åç§°"""
    return CLASS_NAMES[label]


def generate_training_report(model_name, history, train_acc, test_acc, X_train, y_train, 
                             X_test, y_test, model, layer_info, learning_rate, training_time=None):
    """
    ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå¹¶ä¿å­˜åˆ°æ–‡ä»¶
    
    å‚æ•°:
        model_name: æ¨¡å‹åç§°
        history: è®­ç»ƒå†å²
        train_acc: æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡
        test_acc: æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_test, y_test: æµ‹è¯•æ•°æ®
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        layer_info: ç½‘ç»œå±‚ä¿¡æ¯
        learning_rate: å­¦ä¹ ç‡
        training_time: è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    import numpy as np
    from datetime import datetime
    
    # åˆ›å»ºæŠ¥å‘Šå†…å®¹
    report_lines = []
    
    def add_line(text):
        """æ·»åŠ ä¸€è¡Œåˆ°æŠ¥å‘Šå¹¶æ‰“å°"""
        print(text)
        report_lines.append(text)
    
    add_line("\n" + "=" * 70)
    add_line(" " * 20 + f"{model_name} è®­ç»ƒæŠ¥å‘Š")
    add_line("=" * 70)
    
    # æ·»åŠ ç”Ÿæˆæ—¶é—´
    add_line(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. æ¨¡å‹é…ç½®ä¿¡æ¯
    add_line("\nã€æ¨¡å‹é…ç½®ã€‘")
    add_line("-" * 70)
    add_line(f"  ç½‘ç»œç»“æ„: {layer_info}")
    add_line(f"  è®­ç»ƒè½®æ•°: {history['epochs']}")
    add_line(f"  æ‰¹æ¬¡å¤§å°: {history['batch_size']}")
    add_line(f"  å­¦ä¹ ç‡: {learning_rate}")
    if training_time:
        add_line(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
    
    # 2. æ•°æ®é›†ä¿¡æ¯
    add_line("\nã€æ•°æ®é›†ä¿¡æ¯ã€‘")
    add_line("-" * 70)
    add_line(f"  è®­ç»ƒæ ·æœ¬æ•°: {X_train.shape[0]:,}")
    add_line(f"  æµ‹è¯•æ ·æœ¬æ•°: {X_test.shape[0]:,}")
    add_line(f"  è¾“å…¥ç»´åº¦: {X_train.shape[1]}")
    add_line(f"  ç±»åˆ«æ•°é‡: 10")
    
    # 3. è®­ç»ƒè¿‡ç¨‹
    add_line("\nã€è®­ç»ƒè¿‡ç¨‹ã€‘")
    add_line("-" * 70)
    add_line(f"  åˆå§‹è®­ç»ƒå‡†ç¡®ç‡: {history['train_acc'][0]:.4f} ({history['train_acc'][0]*100:.2f}%)")
    add_line(f"  åˆå§‹æµ‹è¯•å‡†ç¡®ç‡: {history['test_acc'][0]:.4f} ({history['test_acc'][0]*100:.2f}%)")
    add_line(f"  æœ€é«˜è®­ç»ƒå‡†ç¡®ç‡: {max(history['train_acc']):.4f} ({max(history['train_acc'])*100:.2f}%)")
    add_line(f"  æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡: {max(history['test_acc']):.4f} ({max(history['test_acc'])*100:.2f}%)")
    add_line(f"  å‡†ç¡®ç‡æå‡: {(history['test_acc'][-1] - history['test_acc'][0]):.4f} "
          f"({(history['test_acc'][-1] - history['test_acc'][0])*100:.2f}%)")
    
    # æ·»åŠ æ¯ä¸ªepochçš„è¯¦ç»†æ•°æ®
    add_line("\n  å„è½®æ¬¡è¯¦ç»†æ•°æ®:")
    add_line("  " + "-" * 50)
    add_line(f"  {'Epoch':<8} {'è®­ç»ƒå‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15}")
    add_line("  " + "-" * 50)
    for i in range(len(history['train_acc'])):
        add_line(f"  {i+1:<8} {history['train_acc'][i]:.4f} ({history['train_acc'][i]*100:.2f}%)   "
                f"{history['test_acc'][i]:.4f} ({history['test_acc'][i]*100:.2f}%)")
    
    # 4. æœ€ç»ˆæ€§èƒ½
    add_line("\nã€æœ€ç»ˆæ€§èƒ½ã€‘")
    add_line("-" * 70)
    add_line(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)")
    add_line(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc*100:.2f}%)")
    add_line(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {(train_acc - test_acc):.4f} ({(train_acc - test_acc)*100:.2f}%)")
    
    # 5. æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
    add_line("\nã€å„ç±»åˆ«æ€§èƒ½ã€‘")
    add_line("-" * 70)
    predictions = model.predict(X_test)
    class_correct = np.zeros(10)
    class_total = np.zeros(10)
    
    for i in range(len(y_test)):
        label = y_test[i]
        class_total[label] += 1
        if predictions[i] == label:
            class_correct[label] += 1
    
    add_line(f"  {'ç±»åˆ«':<15} {'å‡†ç¡®ç‡':<10} {'æ­£ç¡®/æ€»æ•°'}")
    add_line("  " + "-" * 40)
    for i in range(10):
        acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        add_line(f"  {get_class_name(i):<15} {acc:.4f}     {int(class_correct[i])}/{int(class_total[i])}")
    
    # 6. é¢„æµ‹ç¤ºä¾‹
    add_line("\nã€é¢„æµ‹ç¤ºä¾‹ã€‘")
    add_line("-" * 70)
    sample_indices = np.random.choice(len(X_test), 10, replace=False)
    correct_count = 0
    for idx in sample_indices:
        pred = model.predict(X_test[idx:idx+1])[0]
        true_label = y_test[idx]
        is_correct = pred == true_label
        if is_correct:
            correct_count += 1
        status = "âœ“" if is_correct else "âœ—"
        add_line(f"  æ ·æœ¬ #{idx:5d}: çœŸå®={get_class_name(true_label):15s} | "
              f"é¢„æµ‹={get_class_name(pred):15s} {status}")
    add_line(f"\n  éšæœºæ ·æœ¬å‡†ç¡®ç‡: {correct_count}/10")
    
    # 7. æ€»ç»“
    add_line("\nã€æ€»ç»“ã€‘")
    add_line("-" * 70)
    if test_acc >= 0.90:
        performance = "ä¼˜ç§€"
    elif test_acc >= 0.85:
        performance = "è‰¯å¥½"
    elif test_acc >= 0.80:
        performance = "ä¸€èˆ¬"
    else:
        performance = "éœ€è¦æ”¹è¿›"
    
    add_line(f"  æ¨¡å‹æ€§èƒ½è¯„çº§: {performance}")
    add_line(f"  è®­ç»ƒçŠ¶æ€: {'å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ' if (train_acc - test_acc) > 0.05 else 'è®­ç»ƒæ­£å¸¸'}")
    
    suggestion_text = ""
    if (train_acc - test_acc) > 0.05:
        suggestion_text = "è€ƒè™‘æ·»åŠ æ­£åˆ™åŒ–æˆ–dropoutæ¥å‡å°‘è¿‡æ‹Ÿåˆ"
    elif test_acc < 0.85:
        suggestion_text = "å¯ä»¥å°è¯•å¢åŠ ç½‘ç»œæ·±åº¦ã€è°ƒæ•´å­¦ä¹ ç‡æˆ–è®­ç»ƒæ›´å¤šè½®æ¬¡"
    else:
        suggestion_text = "æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨"
    add_line(f"  å»ºè®®: {suggestion_text}")
    
    add_line("\n" + "=" * 70)
    add_line(" " * 25 + "æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    add_line("=" * 70 + "\n")
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    # åˆ›å»ºreportsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    reports_dir = "reports"
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šæ¨¡å‹åç§°_æ—¶é—´æˆ³.txt
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    # æ¸…ç†æ¨¡å‹åç§°ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
    clean_model_name = model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')
    filename = os.path.join(reports_dir, f"{clean_model_name}_{timestamp}.txt")
    
    # å†™å…¥æ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}\n")
    
    return filename  # è¿”å›æŠ¥å‘Šæ–‡ä»¶è·¯å¾„


def parse_report_file(report_file):
    """
    ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è§£æè®­ç»ƒç»“æœ
    
    å‚æ•°:
        report_file: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        åŒ…å«è§£æç»“æœçš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    try:
        with open(report_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        result = {}
        
        # è§£ææµ‹è¯•å‡†ç¡®ç‡ï¼ˆä¼˜å…ˆåŒ¹é…ã€æœ€ç»ˆæ€§èƒ½ã€‘éƒ¨åˆ†çš„ï¼Œå¦‚æœæ²¡æœ‰åˆ™åŒ¹é…å…¶ä»–éƒ¨åˆ†ï¼‰
        import re
        # å…ˆå°è¯•åŒ¹é…ã€æœ€ç»ˆæ€§èƒ½ã€‘éƒ¨åˆ†çš„å‡†ç¡®ç‡ï¼ˆæ›´å‡†ç¡®ï¼‰
        final_perf_match = re.search(r'ã€æœ€ç»ˆæ€§èƒ½ã€‘.*?æµ‹è¯•å‡†ç¡®ç‡:\s*([\d.]+)\s*\(([\d.]+)%\)', content, re.DOTALL)
        if final_perf_match:
            result['test_acc'] = float(final_perf_match.group(1))
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…å…¶ä»–éƒ¨åˆ†çš„æµ‹è¯•å‡†ç¡®ç‡
            test_acc_match = re.search(r'æµ‹è¯•å‡†ç¡®ç‡:\s*([\d.]+)\s*\(([\d.]+)%\)', content)
            if test_acc_match:
                result['test_acc'] = float(test_acc_match.group(1))
        
        # è§£æè®­ç»ƒå‡†ç¡®ç‡ï¼ˆä¼˜å…ˆåŒ¹é…ã€æœ€ç»ˆæ€§èƒ½ã€‘éƒ¨åˆ†çš„ï¼‰
        final_train_match = re.search(r'ã€æœ€ç»ˆæ€§èƒ½ã€‘.*?è®­ç»ƒå‡†ç¡®ç‡:\s*([\d.]+)\s*\(([\d.]+)%\)', content, re.DOTALL)
        if final_train_match:
            result['train_acc'] = float(final_train_match.group(1))
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…å…¶ä»–éƒ¨åˆ†çš„è®­ç»ƒå‡†ç¡®ç‡
            train_acc_match = re.search(r'è®­ç»ƒå‡†ç¡®ç‡:\s*([\d.]+)\s*\(([\d.]+)%\)', content)
            if train_acc_match:
                result['train_acc'] = float(train_acc_match.group(1))
        
        # è§£æè®­ç»ƒæ—¶é—´ï¼ˆåŒ¹é… "è®­ç»ƒæ—¶é—´: X.XX ç§’" æˆ– "è®­ç»ƒæ—¶é—´: X.XX ç§’ (X.XX åˆ†é’Ÿ)"ï¼‰
        time_match = re.search(r'è®­ç»ƒæ—¶é—´:\s*([\d.]+)\s*ç§’', content)
        if time_match:
            result['training_time'] = float(time_match.group(1))
        
        return result if result else None
    except Exception as e:
        print(f"è§£ææŠ¥å‘Šæ–‡ä»¶ {report_file} æ—¶å‡ºé”™: {e}")
        return None


def generate_summary_report(model_results, reports_dir="reports"):
    """
    ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        model_results: å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹çš„ç»“æœä¿¡æ¯
            [{
                'model_name': æ¨¡å‹åç§°,
                'train_acc': è®­ç»ƒå‡†ç¡®ç‡,
                'test_acc': æµ‹è¯•å‡†ç¡®ç‡,
                'training_time': è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰,
                'status': 'success' æˆ– 'failed',
                'error': é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
            }, ...]
        reports_dir: æŠ¥å‘Šç›®å½•
    """
    from datetime import datetime
    import glob
    
    # åˆ›å»ºæŠ¥å‘Šå†…å®¹
    report_lines = []
    
    def add_line(text):
        """æ·»åŠ ä¸€è¡Œåˆ°æŠ¥å‘Šå¹¶æ‰“å°"""
        print(text)
        report_lines.append(text)
    
    add_line("\n" + "=" * 70)
    add_line(" " * 20 + "æ‰€æœ‰æ¨¡å‹è®­ç»ƒæ±‡æ€»æŠ¥å‘Š")
    add_line("=" * 70)
    
    # æ·»åŠ ç”Ÿæˆæ—¶é—´
    add_line(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_models = len(model_results)
    success_models = [r for r in model_results if r.get('status') == 'success']
    failed_models = [r for r in model_results if r.get('status') == 'failed']
    
    add_line("\nã€è¿è¡Œç»Ÿè®¡ã€‘")
    add_line("-" * 70)
    add_line(f"  æ€»æ¨¡å‹æ•°: {total_models}")
    add_line(f"  æˆåŠŸ: {len(success_models)} ä¸ª")
    add_line(f"  å¤±è´¥: {len(failed_models)} ä¸ª")
    
    # åœ¨ç”Ÿæˆå¯¹æ¯”è¡¨ä¹‹å‰ï¼Œå…ˆå°è¯•ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è¡¥å……å‡†ç¡®ç‡ä¿¡æ¯
    if os.path.exists(reports_dir):
        report_files = sorted(glob.glob(os.path.join(reports_dir, "*.txt")), 
                             key=os.path.getmtime, reverse=True)
        
        # å»ºç«‹æ¨¡å‹åç§°æ˜ å°„ï¼ˆå¤„ç†ä¸­æ–‡æ¨¡å‹åç§°å’Œè‹±æ–‡æ–‡ä»¶åçš„åŒ¹é…ï¼‰
        model_name_mapping = {
            'å¤šå±‚æ„ŸçŸ¥æœº (MLP)': ['å¤šå±‚æ„ŸçŸ¥æœº_MLP', 'MLP', 'å¤šå±‚æ„ŸçŸ¥æœº'],
            'å·ç§¯ç¥ç»ç½‘ç»œ (CNN)': ['å·ç§¯ç¥ç»ç½‘ç»œ_CNN', 'CNN', 'å·ç§¯ç¥ç»ç½‘ç»œ'],
            'æ®‹å·®ç½‘ç»œ (ResNet)': ['æ®‹å·®ç¥ç»ç½‘ç»œ_ResNet', 'ResNet', 'æ®‹å·®ç½‘ç»œ', 'æ®‹å·®ç¥ç»ç½‘ç»œ'],
            'LeNet-5': ['LeNet', 'LeNet-5'],
            'Wide ResNet-28-10 + Random Erasing': ['Wide_ResNet', 'Wide ResNet'],
            'DenseNet-BC': ['DenseNet', 'DenseNet-BC'],
            'Capsule Network': ['Capsule_Network', 'Capsule', 'CapsNet']
        }
        
        # ä¸ºæ¯ä¸ªæˆåŠŸæ¨¡å‹æŸ¥æ‰¾å¯¹åº”çš„æŠ¥å‘Šæ–‡ä»¶å¹¶è§£æ
        model_file_map = {}
        for file in report_files:
            basename = os.path.basename(file)
            # è·³è¿‡æ±‡æ€»æŠ¥å‘Šå’Œå‚æ•°æ–‡ä»¶
            if 'æ±‡æ€»æŠ¥å‘Š' in basename or 'å‚æ•°' in basename:
                continue
            # ä»æ–‡ä»¶åæå–æ¨¡å‹åç§°ï¼ˆå»æ‰æ—¶é—´æˆ³ï¼‰
            parts = basename.rsplit('_', 2)
            if len(parts) >= 3:
                model_key = '_'.join(parts[:-2])
                if model_key not in model_file_map:
                    model_file_map[model_key] = file
        
        # æ›´æ–°æ¨¡å‹ç»“æœï¼ˆä¼˜å…ˆä½¿ç”¨æŠ¥å‘Šæ–‡ä»¶ä¸­çš„å‡†ç¡®ç‡ï¼‰
        for result in model_results:
            if result.get('status') == 'success':
                model_name = result.get('model_name', '')
                
                # æŸ¥æ‰¾åŒ¹é…çš„æŠ¥å‘Šæ–‡ä»¶
                matched_file = None
                # é¦–å…ˆå°è¯•ç›´æ¥åŒ¹é…æ–‡ä»¶åä¸­çš„å…³é”®éƒ¨åˆ†ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
                for key, file_path in model_file_map.items():
                    # æå–æ¨¡å‹åç§°çš„å…³é”®è¯è¿›è¡ŒåŒ¹é…ï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…é€»è¾‘ï¼‰
                    matched = False
                    
                    if 'MLP' in model_name or 'å¤šå±‚æ„ŸçŸ¥æœº' in model_name:
                        # MLP: åªåŒ¹é…åŒ…å«"MLP"æˆ–"å¤šå±‚æ„ŸçŸ¥æœº"ä½†ä¸åŒ…å«å…¶ä»–å¤æ‚æ¨¡å‹çš„åç§°
                        matched = ('MLP' in key or 'å¤šå±‚æ„ŸçŸ¥æœº' in key) and 'Wide' not in key and 'Dense' not in key
                    elif 'CNN' in model_name or 'å·ç§¯ç¥ç»ç½‘ç»œ' in model_name:
                        # CNN: åªåŒ¹é…åŒ…å«"CNN"æˆ–"å·ç§¯ç¥ç»ç½‘ç»œ"çš„æ–‡ä»¶
                        matched = ('CNN' in key or 'å·ç§¯ç¥ç»ç½‘ç»œ' in key) and 'Wide' not in key
                    elif 'æ®‹å·®ç½‘ç»œ' in model_name or ('ResNet' in model_name and 'Wide' not in model_name):
                        # ResNet: åªåŒ¹é…"æ®‹å·®ç¥ç»ç½‘ç»œ"æˆ–åªåŒ…å«"ResNet"ï¼ˆä¸åŒ…å«"Wide"ï¼‰çš„æ–‡ä»¶
                        matched = ('æ®‹å·®' in key and 'Wide' not in key) or (key == 'ResNet' or (key.endswith('ResNet') and not key.startswith('Wide')))
                    elif 'LeNet' in model_name:
                        # LeNet: åªåŒ¹é…LeNetç›¸å…³çš„æ–‡ä»¶
                        matched = 'LeNet' in key
                    elif 'Wide' in model_name or 'WRN' in model_name:
                        # Wide ResNet: åªåŒ¹é…Wide ResNetç›¸å…³çš„æ–‡ä»¶
                        matched = 'Wide' in key or 'WRN' in key
                    elif 'DenseNet' in model_name or ('Dense' in model_name and 'DenseNet' in model_name):
                        # DenseNet: åªåŒ¹é…DenseNetç›¸å…³çš„æ–‡ä»¶
                        matched = 'DenseNet' in key or ('Dense' in key and 'Wide' not in key)
                    elif 'Capsule' in model_name or 'CapsNet' in model_name:
                        # Capsule Network: åªåŒ¹é…Capsuleç›¸å…³çš„æ–‡ä»¶
                        matched = 'Capsule' in key or 'CapsNet' in key
                    
                    if matched:
                        matched_file = file_path
                        break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨æ˜ å°„è¡¨
                if not matched_file:
                    for full_name, keywords in model_name_mapping.items():
                        if full_name == model_name:
                            for kw in keywords:
                                for key, file_path in model_file_map.items():
                                    if kw in key:
                                        matched_file = file_path
                                        break
                                if matched_file:
                                    break
                        if matched_file:
                            break
                
                # è§£æåŒ¹é…çš„æŠ¥å‘Šæ–‡ä»¶å¹¶æ›´æ–°ç»“æœ
                if matched_file:
                    parsed = parse_report_file(matched_file)
                    if parsed:
                        # ä¼˜å…ˆä½¿ç”¨æŠ¥å‘Šæ–‡ä»¶ä¸­çš„æ•°æ®ï¼ˆæ›´å‡†ç¡®ï¼‰
                        if 'test_acc' in parsed:
                            result['test_acc'] = parsed['test_acc']
                        if 'train_acc' in parsed:
                            result['train_acc'] = parsed['train_acc']
                        if 'training_time' in parsed and parsed['training_time'] > 0:
                            result['training_time'] = parsed['training_time']
                        print(f"  å·²ä»æŠ¥å‘Šæ–‡ä»¶æ›´æ–° {model_name} çš„å‡†ç¡®ç‡: è®­ç»ƒ={result.get('train_acc', 0):.4f}, æµ‹è¯•={result.get('test_acc', 0):.4f}")
    
    # æˆåŠŸæ¨¡å‹å¯¹æ¯”
    if success_models:
        add_line("\nã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‘")
        add_line("-" * 70)
        add_line(f"  {'æ¨¡å‹åç§°':<25} {'è®­ç»ƒå‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15} {'è®­ç»ƒæ—¶é—´':<15}")
        add_line("  " + "-" * 68)
        
        # æŒ‰æµ‹è¯•å‡†ç¡®ç‡æ’åº
        sorted_models = sorted(success_models, key=lambda x: x.get('test_acc', 0), reverse=True)
        
        for result in sorted_models:
            model_name = result.get('model_name', 'æœªçŸ¥')
            train_acc = result.get('train_acc', 0)
            test_acc = result.get('test_acc', 0)
            training_time = result.get('training_time', 0)
            
            train_acc_str = f"{train_acc:.4f} ({train_acc*100:.2f}%)"
            test_acc_str = f"{test_acc:.4f} ({test_acc*100:.2f}%)"
            
            if training_time > 0:
                if training_time < 60:
                    time_str = f"{training_time:.1f}ç§’"
                else:
                    time_str = f"{training_time/60:.1f}åˆ†é’Ÿ"
            else:
                time_str = "æœªçŸ¥"
            
            add_line(f"  {model_name:<25} {train_acc_str:<15} {test_acc_str:<15} {time_str:<15}")
        
        # æœ€ä½³æ¨¡å‹
        if sorted_models:
            best = sorted_models[0]
            add_line(f"\n  ğŸ† æœ€ä½³æ¨¡å‹: {best.get('model_name')} (æµ‹è¯•å‡†ç¡®ç‡: {best.get('test_acc', 0):.4f})")
    
    # å¤±è´¥æ¨¡å‹
    if failed_models:
        add_line("\nã€å¤±è´¥æ¨¡å‹ã€‘")
        add_line("-" * 70)
        for result in failed_models:
            model_name = result.get('model_name', 'æœªçŸ¥')
            error = result.get('error', 'æœªçŸ¥é”™è¯¯')
            add_line(f"  âœ— {model_name}: {error}")
    
    # æŠ¥å‘Šæ–‡ä»¶åˆ—è¡¨
    add_line("\nã€ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶ã€‘")
    add_line("-" * 70)
    
    # æŸ¥æ‰¾æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶
    if os.path.exists(reports_dir):
        report_files = sorted(glob.glob(os.path.join(reports_dir, "*.txt")), 
                             key=os.path.getmtime, reverse=True)
        
        # åªæ˜¾ç¤ºæœ€è¿‘çš„æŠ¥å‘Šï¼ˆæ¯ä¸ªæ¨¡å‹ä¸€ä¸ªï¼‰
        model_files = {}
        for file in report_files:
            # ä»æ–‡ä»¶åæå–æ¨¡å‹åç§°ï¼ˆå»æ‰æ—¶é—´æˆ³ï¼‰
            basename = os.path.basename(file)
            # è·³è¿‡æ±‡æ€»æŠ¥å‘Šï¼ˆä¼šåœ¨æœ€åå•ç‹¬æ˜¾ç¤ºï¼‰
            if 'æ±‡æ€»æŠ¥å‘Š' in basename:
                continue
            # æ‰¾åˆ°æœ€åä¸€ä¸ªä¸‹åˆ’çº¿çš„ä½ç½®ï¼ˆæ—¶é—´æˆ³å‰ï¼‰
            parts = basename.rsplit('_', 2)
            if len(parts) >= 3:
                model_key = '_'.join(parts[:-2])  # å»æ‰æ—¶é—´æˆ³éƒ¨åˆ†
                if model_key not in model_files:
                    model_files[model_key] = file
        
        if model_files:
            for model_key, file_path in sorted(model_files.items()):
                file_size = os.path.getsize(file_path)
                file_size_kb = file_size / 1024
                add_line(f"  âœ“ {os.path.basename(file_path)} ({file_size_kb:.1f} KB)")
        else:
            add_line("  æœªæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶")
    else:
        add_line("  æŠ¥å‘Šç›®å½•ä¸å­˜åœ¨")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°æ–‡ä»¶
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_filename = os.path.join(reports_dir, f"æ±‡æ€»æŠ¥å‘Š_{timestamp}.txt")
    
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    # åœ¨æŠ¥å‘Šä¸­æ·»åŠ æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶ä¿¡æ¯
    add_line(f"\n  ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {os.path.basename(summary_filename)}")
    
    add_line("\n" + "=" * 70)
    add_line(" " * 25 + "æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    add_line("=" * 70 + "\n")
    
    print(f"\nâœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_filename}\n")
    
    return summary_filename

